---
title: 音視頻基本處理流程
description: 音視頻資料從來源容器讀取、解封裝、（必要時）編解碼、到封裝成新容器的基本處理流程，並說明純轉封裝與跨 codec 轉換的差異。
authros: yhchen
tags:
  - streaming
date: 2025-12-08
---

從素材進入系統，到畫面出現在螢幕上的完整路徑，其中所需要的資料結構、格式、傳輸協定，其中包含的所有元件就是音視訊的處理路徑。

| ![GST](https://gstreamer.freedesktop.org/documentation/application-development/introduction/images/simple-player.png) |
| :-------------------------------------------------------------------------------------------------------------------: |
|                                               GStreamer Pipeline 示例圖                                               |

<!-- truncate -->

## 採集 Sampling

這是音視頻的起點，硬體把連續的物理世界（聲音壓力、光強度）轉成離散的數位數字。Frame 是畫面與聲音的最原始形態。影像是像素矩陣，音訊是連續取樣。這些資料量很大，通常不會直接送到網路上。

- 取樣：每隔固定時間抓一個數值（sample）。
- 量化：把這個數值四捨五入到某個 bit 深度。
- 編碼：用二進位格式把這些數字排好。

對於音訊來說，類比轉數位常見的採樣率是 44.1KHz 或是 48KHz，將每次量測的數值量化成一個 16bit 或是 24bit 的整數，這個矩陣就代表一秒的聲音；對於視訊來說，常常講「Frame rate」，相機每秒拍幾張畫面、每個畫面中的像素用幾 bits 去表示等。這個過程就是採樣。

取樣率太低會導致音訊會失真、出現 aliasing；影像會看起來卡、殘影。
取樣率太高則導致資料量卻暴增，影響後面整條鏈的頻寬與算力。

## 編碼與解碼 Codec

編碼負責壓縮，解碼負責解壓縮。就是進行 frame 與 bitstream 之間的轉換。

壓縮後的資料可以縮小 10～100 倍，傳輸才變得可行。H264、H265、VP8、AV1、AAC、Opus 都屬於這一層。

影像編碼的行為包含：

- 參考前後畫面預測
- 建立 motion vector
- 使用 I / P / B frame 減少重複資訊

影像解碼的行為包含：

- 讀取 bitstream header，了解 frame 類型與參考資訊
- 重建 prediction + residual
- 做反量化與反 transform
- 組回完整 frame 或 PCM 波形

编解碼本身不關心「bitstream 的來源」，不管是 TS、MP4、RTP，它只吃「乾淨的 bitstream」。

對於即時通訊，codec 設定（bitrate、GOP、frame size、packetization）直接影響延遲、畫質、丟包後的恢復能力。

## 封裝與解封裝 Demuxing/Muxing

當音視訊進行編碼後，每個 bitstream 稱為 track(軌道)。而將一個或多個軌道組成一個可以傳輸、儲存的串流，就是封裝。

- 收到一條或多條已壓縮的 bitstream（例如 H264 + AAC）。
- 決定要用什麼封裝格式：MP4、TS、WebM、RTP、FLV…
- 依照該格式的規則，把 video/audio/subtitle/metadata 混進同一個容器。
- 安排時間戳與封包邊界，讓未來解封裝能準確還原。

舉例來說，輸入 H264 + AAC 兩個碼流，最終可能輸出 `.mp4`, `.ts`, `.webm` 檔案等，這個過程就是封裝。

反過來說，將封裝給打開，還原出裡面的所有軌道，就是解封裝。解封裝不需要理解編碼的細節，只要確認 Frame 的邊界正確、時間戳正確、各條軌道的順序正確還原即可。

### 媒體容器

容器是一種結構化的包裝方式，幫你記錄 sample 的邊界、時間戳、編碼參數和軌道分類，讓播放端能知道哪一段是畫面、哪一段是聲音，以及該如何按時間組合。

管理多條 tracks

- 影像軌（例如 H264 bitstream）
- 音訊軌（例如 AAC、Opus、Vorbis）
- 有時還會有字幕、章節、附加 metadata

標記 sample 邊界與順序

- 什麼地方開始是一個完整的 frame
- 哪些 sample 對應什麼時間戳（PTS/DTS）
- 不同軌道如何對齊時間

常見的容器有 `mp4`, `mkv`, `webm`, `flv`, `mpeg-ts`, `ogg`, `avi` 等。而 metadata 是容器用來描述內容結構、時間軸與播放資訊的輔助資料。它不承載影像或音訊本體，但讓播放器知道「這段內容該怎麼被正確解讀」。

- 軌道資訊
- 時間軸與取樣表
- 編碼器設定
- 索引
- 標籤與描述
    - 標題、作者、語言、日期或封面圖
    - 通常是給影音平台用的

## 範例說明

以最上面的 GStreamer Pipeline 來解釋，假定我們輸入一個 `.ogg` 檔案，並且打算輸出 `.mkv` 檔案，並且修改音視訊編碼：

1. 把 MP4 的 metadata 讀進來
    - 有幾個 track
    - 每個 track 的時間軸與 frame 邊界
    - 編解碼的類型 Vorbis + Theora
2. 解封裝
    - 找到下一個 video/audio sample 的位置
    - 依照時間戳 PTS/DTS 排序
    - 把壓縮後的 Frame 一個一個讀出來
3. 解碼
    - 調用對應的編解碼器，將數據還原成原本的採樣資料
    - 常見的內容是 YUV(Video) + PCM(Audio)
4. 編碼
    - 建立 encoder（例如 libvpx、libopus、libx264 等）
    - 設定參數（bitrate、profile、片段大小、GOP 等）
    - 將 raw frame 壓成新 codec 的 bitstream
5. 封裝
    - Muxer 會決定要怎麼把這兩條流放進 MKV
    - 創建 video / audio track
    - 設定時間基準 (timebase)
    - 填寫編碼器的配置資料
6. 輸出 MKV 檔案，且補上
    - 檔案需要的索引
    - metadata

如果我們不打算修改編碼類型，那麼編解碼的步驟就可以省略

## 總結

這就是音視訊處理的基本概念：讀取原始容器的 metadata、依照時間戳拆出壓縮後的 frame（解封裝）、必要時將資料還原為原始採樣（解碼）、視需求重新壓縮成新 codec（編碼）、建立目標容器並依規則寫入每個 sample（封裝），最後生成可播放的媒體容器。
