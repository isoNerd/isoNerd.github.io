<?xml version="1.0" encoding="utf-8"?><?xml-stylesheet type="text/xsl" href="atom.xsl"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://isoNerd.github.io/en/blog</id>
    <title>Vibe 部落格 Blog</title>
    <updated>2025-12-08T00:00:00.000Z</updated>
    <generator>https://github.com/jpmonette/feed</generator>
    <link rel="alternate" href="https://isoNerd.github.io/en/blog"/>
    <subtitle>Vibe 部落格 Blog</subtitle>
    <icon>https://isoNerd.github.io/en/img/favicon.ico</icon>
    <entry>
        <title type="html"><![CDATA[音視頻基本處理流程]]></title>
        <id>https://isoNerd.github.io/en/blog/20251208-01</id>
        <link href="https://isoNerd.github.io/en/blog/20251208-01"/>
        <updated>2025-12-08T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[音視頻資料從來源容器讀取、解封裝、（必要時）編解碼、到封裝成新容器的基本處理流程，並說明純轉封裝與跨 codec 轉換的差異。]]></summary>
        <content type="html"><![CDATA[<p>從素材進入系統，到畫面出現在螢幕上的完整路徑，其中所需要的資料結構、格式、傳輸協定，其中包含的所有元件就是音視訊的處理路徑。</p>
<table><thead><tr><th style="text-align:center"><img decoding="async" loading="lazy" src="https://gstreamer.freedesktop.org/documentation/application-development/introduction/images/simple-player.png" alt="GST" class="img_pNQo"></th></tr></thead><tbody><tr><td style="text-align:center">GStreamer Pipeline 示例圖</td></tr></tbody></table>
<h2 class="anchor anchorTargetStickyNavbar_Mpx2" id="採集-sampling">採集 Sampling<a href="https://isonerd.github.io/en/blog/20251208-01#%E6%8E%A1%E9%9B%86-sampling" class="hash-link" aria-label="Direct link to 採集 Sampling" title="Direct link to 採集 Sampling" translate="no">​</a></h2>
<p>這是音視頻的起點，硬體把連續的物理世界（聲音壓力、光強度）轉成離散的數位數字。Frame 是畫面與聲音的最原始形態。影像是像素矩陣，音訊是連續取樣。這些資料量很大，通常不會直接送到網路上。</p>
<ul>
<li class="">取樣：每隔固定時間抓一個數值（sample）。</li>
<li class="">量化：把這個數值四捨五入到某個 bit 深度。</li>
<li class="">編碼：用二進位格式把這些數字排好。</li>
</ul>
<p>對於音訊來說，類比轉數位常見的採樣率是 44.1KHz 或是 48KHz，將每次量測的數值量化成一個 16bit 或是 24bit 的整數，這個矩陣就代表一秒的聲音；對於視訊來說，常常講「Frame rate」，相機每秒拍幾張畫面、每個畫面中的像素用幾 bits 去表示等。這個過程就是採樣。</p>
<p>取樣率太低會導致音訊會失真、出現 aliasing；影像會看起來卡、殘影。
取樣率太高則導致資料量卻暴增，影響後面整條鏈的頻寬與算力。</p>
<h2 class="anchor anchorTargetStickyNavbar_Mpx2" id="編碼與解碼-codec">編碼與解碼 Codec<a href="https://isonerd.github.io/en/blog/20251208-01#%E7%B7%A8%E7%A2%BC%E8%88%87%E8%A7%A3%E7%A2%BC-codec" class="hash-link" aria-label="Direct link to 編碼與解碼 Codec" title="Direct link to 編碼與解碼 Codec" translate="no">​</a></h2>
<p>編碼負責壓縮，解碼負責解壓縮。就是進行 frame 與 bitstream 之間的轉換。</p>
<p>壓縮後的資料可以縮小 10～100 倍，傳輸才變得可行。H264、H265、VP8、AV1、AAC、Opus 都屬於這一層。</p>
<p>影像編碼的行為包含：</p>
<ul>
<li class="">參考前後畫面預測</li>
<li class="">建立 motion vector</li>
<li class="">使用 I / P / B frame 減少重複資訊</li>
</ul>
<p>影像解碼的行為包含：</p>
<ul>
<li class="">讀取 bitstream header，了解 frame 類型與參考資訊</li>
<li class="">重建 prediction + residual</li>
<li class="">做反量化與反 transform</li>
<li class="">組回完整 frame 或 PCM 波形</li>
</ul>
<p>编解碼本身不關心「bitstream 的來源」，不管是 TS、MP4、RTP，它只吃「乾淨的 bitstream」。</p>
<p>對於即時通訊，codec 設定（bitrate、GOP、frame size、packetization）直接影響延遲、畫質、丟包後的恢復能力。</p>
<h2 class="anchor anchorTargetStickyNavbar_Mpx2" id="封裝與解封裝-demuxingmuxing">封裝與解封裝 Demuxing/Muxing<a href="https://isonerd.github.io/en/blog/20251208-01#%E5%B0%81%E8%A3%9D%E8%88%87%E8%A7%A3%E5%B0%81%E8%A3%9D-demuxingmuxing" class="hash-link" aria-label="Direct link to 封裝與解封裝 Demuxing/Muxing" title="Direct link to 封裝與解封裝 Demuxing/Muxing" translate="no">​</a></h2>
<p>當音視訊進行編碼後，每個 bitstream 稱為 track(軌道)。而將一個或多個軌道組成一個可以傳輸、儲存的串流，就是封裝。</p>
<ul>
<li class="">收到一條或多條已壓縮的 bitstream（例如 H264 + AAC）。</li>
<li class="">決定要用什麼封裝格式：MP4、TS、WebM、RTP、FLV…</li>
<li class="">依照該格式的規則，把 video/audio/subtitle/metadata 混進同一個容器。</li>
<li class="">安排時間戳與封包邊界，讓未來解封裝能準確還原。</li>
</ul>
<p>舉例來說，輸入 H264 + AAC 兩個碼流，最終可能輸出 <code>.mp4</code>, <code>.ts</code>, <code>.webm</code> 檔案等，這個過程就是封裝。</p>
<p>反過來說，將封裝給打開，還原出裡面的所有軌道，就是解封裝。解封裝不需要理解編碼的細節，只要確認 Frame 的邊界正確、時間戳正確、各條軌道的順序正確還原即可。</p>
<h3 class="anchor anchorTargetStickyNavbar_Mpx2" id="媒體容器">媒體容器<a href="https://isonerd.github.io/en/blog/20251208-01#%E5%AA%92%E9%AB%94%E5%AE%B9%E5%99%A8" class="hash-link" aria-label="Direct link to 媒體容器" title="Direct link to 媒體容器" translate="no">​</a></h3>
<p>容器是一種結構化的包裝方式，幫你記錄 sample 的邊界、時間戳、編碼參數和軌道分類，讓播放端能知道哪一段是畫面、哪一段是聲音，以及該如何按時間組合。</p>
<p>管理多條 tracks</p>
<ul>
<li class="">影像軌（例如 H264 bitstream）</li>
<li class="">音訊軌（例如 AAC、Opus、Vorbis）</li>
<li class="">有時還會有字幕、章節、附加 metadata</li>
</ul>
<p>標記 sample 邊界與順序</p>
<ul>
<li class="">什麼地方開始是一個完整的 frame</li>
<li class="">哪些 sample 對應什麼時間戳（PTS/DTS）</li>
<li class="">不同軌道如何對齊時間</li>
</ul>
<p>常見的容器有 <code>mp4</code>, <code>mkv</code>, <code>webm</code>, <code>flv</code>, <code>mpeg-ts</code>, <code>ogg</code>, <code>avi</code> 等。而 metadata 是容器用來描述內容結構、時間軸與播放資訊的輔助資料。它不承載影像或音訊本體，但讓播放器知道「這段內容該怎麼被正確解讀」。</p>
<ul>
<li class="">軌道資訊</li>
<li class="">時間軸與取樣表</li>
<li class="">編碼器設定</li>
<li class="">索引</li>
<li class="">標籤與描述<!-- -->
<ul>
<li class="">標題、作者、語言、日期或封面圖</li>
<li class="">通常是給影音平台用的</li>
</ul>
</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Mpx2" id="範例說明">範例說明<a href="https://isonerd.github.io/en/blog/20251208-01#%E7%AF%84%E4%BE%8B%E8%AA%AA%E6%98%8E" class="hash-link" aria-label="Direct link to 範例說明" title="Direct link to 範例說明" translate="no">​</a></h2>
<p>以最上面的 GStreamer Pipeline 來解釋，假定我們輸入一個 <code>.ogg</code> 檔案，並且打算輸出 <code>.mkv</code> 檔案，並且修改音視訊編碼：</p>
<ol>
<li class="">把 MP4 的 metadata 讀進來<!-- -->
<ul>
<li class="">有幾個 track</li>
<li class="">每個 track 的時間軸與 frame 邊界</li>
<li class="">編解碼的類型 Vorbis + Theora</li>
</ul>
</li>
<li class="">解封裝<!-- -->
<ul>
<li class="">找到下一個 video/audio sample 的位置</li>
<li class="">依照時間戳 PTS/DTS 排序</li>
<li class="">把壓縮後的 Frame 一個一個讀出來</li>
</ul>
</li>
<li class="">解碼<!-- -->
<ul>
<li class="">調用對應的編解碼器，將數據還原成原本的採樣資料</li>
<li class="">常見的內容是 YUV(Video) + PCM(Audio)</li>
</ul>
</li>
<li class="">編碼<!-- -->
<ul>
<li class="">建立 encoder（例如 libvpx、libopus、libx264 等）</li>
<li class="">設定參數（bitrate、profile、片段大小、GOP 等）</li>
<li class="">將 raw frame 壓成新 codec 的 bitstream</li>
</ul>
</li>
<li class="">封裝<!-- -->
<ul>
<li class="">Muxer 會決定要怎麼把這兩條流放進 MKV</li>
<li class="">創建 video / audio track</li>
<li class="">設定時間基準 (timebase)</li>
<li class="">填寫編碼器的配置資料</li>
</ul>
</li>
<li class="">輸出 MKV 檔案，且補上<!-- -->
<ul>
<li class="">檔案需要的索引</li>
<li class="">metadata</li>
</ul>
</li>
</ol>
<p>如果我們不打算修改編碼類型，那麼編解碼的步驟就可以省略</p>
<h2 class="anchor anchorTargetStickyNavbar_Mpx2" id="總結">總結<a href="https://isonerd.github.io/en/blog/20251208-01#%E7%B8%BD%E7%B5%90" class="hash-link" aria-label="Direct link to 總結" title="Direct link to 總結" translate="no">​</a></h2>
<p>這就是音視訊處理的基本概念：讀取原始容器的 metadata、依照時間戳拆出壓縮後的 frame（解封裝）、必要時將資料還原為原始採樣（解碼）、視需求重新壓縮成新 codec（編碼）、建立目標容器並依規則寫入每個 sample（封裝），最後生成可播放的媒體容器。</p>]]></content>
        <category label="Streaming 串流應用開發" term="Streaming 串流應用開發"/>
    </entry>
    <entry>
        <title type="html"><![CDATA[幀(Frame) 與 時間戳(Timestamp)]]></title>
        <id>https://isoNerd.github.io/en/blog/20251208-02</id>
        <link href="https://isoNerd.github.io/en/blog/20251208-02"/>
        <updated>2025-12-08T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[幀的基本結構與它在傳輸流程中的定位。]]></summary>
        <content type="html"><![CDATA[<p>音訊與視訊的資料型態雖然不同，但在所有現代串流技術中，它們遵循同一套邏輯：資料以 Frame 為最小單位、時間以 timestamp 表示、兩者配合播放器的時脈建立同步。</p>
<h2 class="anchor anchorTargetStickyNavbar_Mpx2" id="視訊幀video-frame">視訊幀(Video Frame)<a href="https://isonerd.github.io/en/blog/20251208-02#%E8%A6%96%E8%A8%8A%E5%B9%80video-frame" class="hash-link" aria-label="Direct link to 視訊幀(Video Frame)" title="Direct link to 視訊幀(Video Frame)" translate="no">​</a></h2>
<p>影像幀是一個靜態畫面。編碼器、播放器、容器格式都以 Frame 作為處理單位。實務上，一個 Frame 具有以下資訊：</p>
<p>影像幀內容：</p>
<ul>
<li class="">尺寸與像素格式（例如 YUV420）</li>
<li class="">播放順序時間（PTS）</li>
<li class="">解碼順序時間（DTS，有些格式才需要）</li>
<li class="">是否為關鍵畫面（I-frame）或預測畫面</li>
</ul>
<p>Frame 本身不帶「秒數」，只帶 timestamp。秒數是由「timestamp ÷ timebase」換算出來。</p>
<p>例：一個 H264 stream 的時間基常見為 1 / 9K。若某 Frame 的 Timestamp 是 18K，代表它應在「18K / 9K = 2 秒」播放。</p>
<h2 class="anchor anchorTargetStickyNavbar_Mpx2" id="音訊幀audio-frame">音訊幀(Audio Frame)<a href="https://isonerd.github.io/en/blog/20251208-02#%E9%9F%B3%E8%A8%8A%E5%B9%80audio-frame" class="hash-link" aria-label="Direct link to 音訊幀(Audio Frame)" title="Direct link to 音訊幀(Audio Frame)" translate="no">​</a></h2>
<p>音訊幀是一段固定長度的取樣資料。聲音在時間軸上是連續的訊號，因此編碼前會先被切成多段，每段形成一個 Audio Frame。每個音訊編碼器會定義自己的 frame duration，例如 AAC 常見 1024 samples，Opus 常見 20ms 或 60ms。這些都會反映在 timestamp 的連續性上。</p>
<p>音訊幀內容：</p>
<ul>
<li class="">一段樣本序列（PCM 或編碼後的 bitstream）</li>
<li class="">取樣率與 channel layout</li>
<li class="">播放時間戳（PTS）</li>
<li class="">編碼器內部需要的附加資訊，例如 Opus 的 TOC byte</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Mpx2" id="時間戳timestamp-與-時基timebase">時間戳(Timestamp) 與 時基(Timebase)<a href="https://isonerd.github.io/en/blog/20251208-02#%E6%99%82%E9%96%93%E6%88%B3timestamp-%E8%88%87-%E6%99%82%E5%9F%BAtimebase" class="hash-link" aria-label="Direct link to 時間戳(Timestamp) 與 時基(Timebase)" title="Direct link to 時間戳(Timestamp) 與 時基(Timebase)" translate="no">​</a></h2>
<p>Timestamp 是一個遞增的整數。它本身沒有意義，需要 timebase 才能換算為秒數。timebase 通常由編碼器、容器或 RTP profile 定義。不同階段可能使用不同的 timebase：</p>
<ul>
<li class="">H264 NALU 在 Annex B 階段沒有時間資訊</li>
<li class="">封裝到 MP4 時會被賦予 timebase，例如 1/90000</li>
<li class="">RTP 傳輸同樣使用 90kHz 作為 video timestamp 的基準</li>
</ul>
<p>若兩個系統的 timebase 不一致，就需要在重封裝時重新計算 timestamp，使播放時序維持一致。</p>
<div class="theme-admonition theme-admonition-tip admonition_BkEo alert alert--success"><div class="admonitionHeading_swSb"><span class="admonitionIcon_nDpx"><svg viewBox="0 0 12 16"><path fill-rule="evenodd" d="M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"></path></svg></span>tip</div><div class="admonitionContent_B2hP"><p>時基的意義是「每一個 timestamp 單位代表多少秒」。</p><p>如果有安裝 ffmpeg，可以嘗試對一個影片進行</p><ul>
<li class=""><code>ffplay -vf "settb=AVTB*2,setpts=PTS" input.mp4</code> : 2倍時基撥放</li>
<li class=""><code>ffplay -vf "settb=AVTB/2,setpts=PTS" input.mp4</code> : 0.5倍時基撥放</li>
</ul></div></div>
<h2 class="anchor anchorTargetStickyNavbar_Mpx2" id="pts-與-dts">PTS 與 DTS<a href="https://isonerd.github.io/en/blog/20251208-02#pts-%E8%88%87-dts" class="hash-link" aria-label="Direct link to PTS 與 DTS" title="Direct link to PTS 與 DTS" translate="no">​</a></h2>
<p>PTS 表示播放順序。DTS 表示解碼順序。存在 B-frame(雙向預測幀) 的編碼器會遇到「解碼順序與播放順序不一致」的情況，因此 DTS 會落後於 PTS。沒有 B-frame 的低延遲設定不需要 DTS，播放器可直接依 PTS 進行排程。這是因為 B-Frame 同時參考了前一幀與後一幀的資訊，因此解碼順序與播放順序可能會不一樣。</p>
<p>例：在一段含 B-frame 的流中：</p>
<ul>
<li class="">Frame 1：I-frame</li>
<li class="">Frame 2：B-frame</li>
<li class="">Frame 3：P-frame</li>
</ul>
<p>解碼順序是 1 → 3 → 2，播放順序是 1 → 2 → 3。PTS/DTS 用來解決這個差異，避免播放器錯誤排序導致畫面停頓。</p>
<h2 class="anchor anchorTargetStickyNavbar_Mpx2" id="frame-與-container-的關係">Frame 與 Container 的關係<a href="https://isonerd.github.io/en/blog/20251208-02#frame-%E8%88%87-container-%E7%9A%84%E9%97%9C%E4%BF%82" class="hash-link" aria-label="Direct link to Frame 與 Container 的關係" title="Direct link to Frame 與 Container 的關係" translate="no">​</a></h2>
<p>Frame 在容器格式中被放進對應的封包。例如：</p>
<ul>
<li class="">在 TS 中是 PES packet。</li>
<li class="">在 MP4 中會形成 sample 並被索引到對應的 track。</li>
<li class="">在 WebM/Matroska 中則是 Block 內的單一 frame。</li>
</ul>
<p>容器不會改變 frame 的內容，但會重新賦予時間資訊，例如：</p>
<ul>
<li class="">sample duration</li>
<li class="">composition time offset（解決 B-frame 重排）</li>
<li class="">decode time</li>
</ul>
<p>重封裝時，timestamp 處理是必要工作。例如把 Annex B H264 封裝進 fMP4，需要重建 sample duration 與 composition time offset，否則播放器無法正確播放。</p>
<p>RTP 不直接傳 Frame，而是將 Frame 拆成多個 RTP packets。每個 packet 帶相同 timestamp，代表它們屬於同一個 Frame。marker bit 通常在最後一個 packet 設定，用來指示 frame boundary。</p>
<p>RTP timestamp 不等於編碼器原始 timestamp。它是以 RFC 規範的 clock rate 遞增：</p>
<ul>
<li class="">Video：90kHz</li>
<li class="">Audio（Opus）：48kHz</li>
<li class="">Audio（AAC）：取樣率</li>
</ul>
<p>若在轉封裝到 RTP 時，沒有正確換算 timestamp，接收端會遭遇 jitter、播放延遲累積或同步錯位。</p>
<h2 class="anchor anchorTargetStickyNavbar_Mpx2" id="可能發生的錯誤">可能發生的錯誤<a href="https://isonerd.github.io/en/blog/20251208-02#%E5%8F%AF%E8%83%BD%E7%99%BC%E7%94%9F%E7%9A%84%E9%8C%AF%E8%AA%A4" class="hash-link" aria-label="Direct link to 可能發生的錯誤" title="Direct link to 可能發生的錯誤" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_Mpx2" id="audiovideo-不同步">Audio/Video 不同步<a href="https://isonerd.github.io/en/blog/20251208-02#audiovideo-%E4%B8%8D%E5%90%8C%E6%AD%A5" class="hash-link" aria-label="Direct link to Audio/Video 不同步" title="Direct link to Audio/Video 不同步" translate="no">​</a></h3>
<p>影像 timestamp 使用 timebase A，音訊使用 timebase B，但兩者沒有換算成共同 timebase 。播放器無法對齊，會表現成「音提早」或「畫面落後」。</p>
<h3 class="anchor anchorTargetStickyNavbar_Mpx2" id="b-frame-設定錯誤">B-frame 設定錯誤<a href="https://isonerd.github.io/en/blog/20251208-02#b-frame-%E8%A8%AD%E5%AE%9A%E9%8C%AF%E8%AA%A4" class="hash-link" aria-label="Direct link to B-frame 設定錯誤" title="Direct link to B-frame 設定錯誤" translate="no">​</a></h3>
<p>若封裝時錯置 composition time offset，B-frame 流會直接破損，例如畫面抖動、播放順序錯亂。</p>]]></content>
        <category label="Streaming 串流應用開發" term="Streaming 串流應用開發"/>
    </entry>
    <entry>
        <title type="html"><![CDATA[我想寫什麼]]></title>
        <id>https://isoNerd.github.io/en/blog/20251116</id>
        <link href="https://isoNerd.github.io/en/blog/20251116"/>
        <updated>2025-11-22T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[這裡大概會變成什麼地方]]></summary>
        <content type="html"><![CDATA[<p>進行開發的時候，偶爾會查詢一些資料，但就是當下看了解決問題，便沒有留下紀錄。之後想要用到時，就會有「這個問題我好像看過，但是忘記在哪裡看到的」，因此我想弄個 Blog 把一些資訊給歸檔。</p>
<p>程式工作本身資訊量大，工具與框架的更新頻率也高。若沒有一個固定位置紀錄思路、操作步驟或小細節，過一段時間就會失去脈絡。這種遺失不只造成時間消耗，也讓後續調整更容易出錯。由於以前當過助教，所以一些文章寫起來可能會有「教科書」的感覺，主要是我希望讓不同程度的人閱讀都很好理解，因此會盡可能把細節寫的詳細。</p>
<p>之後會撰寫的內容應該都是一些技術碎片、測試心得、推論過程，如果性質接近的文章量夠過，會整理成系列；此外也會記一些跟工具、流程、環境架設相關的使用筆記等。總之，這裡應該會慢慢長成一個「我平常在想什麼」的紀錄區域。可能會制定一些方向來撰寫專門的主題文章，目前也還沒有什麼想法，應該也會順便把以前物件導向的教材給搬過來。</p>]]></content>
        <category label="雜項" term="雜項"/>
    </entry>
</feed>